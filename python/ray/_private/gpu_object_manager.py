from typing import Any, Dict, List, Optional, Tuple, TYPE_CHECKING
from ray.actor import ActorHandle
from ray._raylet import ObjectRef

if TYPE_CHECKING:
    import torch


class GPUObjectManager:
    def __init__(self):
        # A dictionary that maps from an object ID to a list of tensors.
        #
        # Note: Currently, `gpu_object_store` is only supported for Ray Actors.
        self.gpu_object_store: Dict[str, List["torch.Tensor"]] = {}
        # A dictionary that maps from an object ref to a tuple of (actor handle, object ref).
        # The key of the dictionary is an object ref that points to data consisting of tensors.
        # These tensors are stored in the GPU object store of the actor referenced by the ActorHandle.
        # The object ref in the tuple is the object ref of a list of tuples, each containing the shape
        # and dtype of a tensor.
        #
        # Note: The coordinator process (i.e., the driver process in most cases), which is responsible
        # for managing out-of-band tensor transfers between actors, uses `gpu_object_refs` to
        # determine whether `ObjectRef`s are stored in an actor's GPU object store or not.
        self.gpu_object_refs: Dict[ObjectRef, Tuple[ActorHandle, ObjectRef]] = {}

    def has_gpu_object(self, obj_id: str) -> bool:
        return obj_id in self.gpu_object_store

    def get_gpu_object(self, obj_id: str) -> Optional[List["torch.Tensor"]]:
        return self.gpu_object_store[obj_id]

    def add_gpu_object(self, obj_id: str, gpu_object: List["torch.Tensor"]):
        self.gpu_object_store[obj_id] = gpu_object

    def remove_gpu_object(self, obj_id: str):
        del self.gpu_object_store[obj_id]

    def add_gpu_object_ref(
        self, obj_ref: ObjectRef, actor_handle: ActorHandle, gpu_object_ref: ObjectRef
    ):
        self.gpu_object_refs[obj_ref] = (actor_handle, gpu_object_ref)

    def remove_gpu_object_ref(self, obj_ref: ObjectRef):
        del self.gpu_object_refs[obj_ref]

    def get_gpu_object_ref(
        self, obj_ref: ObjectRef
    ) -> Optional[Tuple[ActorHandle, ObjectRef]]:
        return self.gpu_object_refs[obj_ref]

    def is_gpu_object_ref(self, obj_ref: ObjectRef) -> bool:
        return obj_ref in self.gpu_object_refs

    def trigger_out_of_band_tensor_transfer(
        self, dst_actor: ActorHandle, task_args: Tuple[Any, ...]
    ):
        """
        Triggers tensor communication operations between actors. When an ObjectRef containing
        in-actor tensors (i.e. ObjectRef exists in `gpu_object_refs`) is passed to another
        actor task, CPU data will still be passed through the object store, but the in-actor
        tensors will be passed out-of-band.

        This function triggers the out-of-band tensor transfer by submitting Ray actor
        tasks `__ray_send__` to the sender actor and `__ray_recv__` to the receiver actor to initiate
        tensor communication using protocols like NCCL or GLOO.

        Before the receiver actor executes the actor task, the deserializer combines the
        CPU data with the tensors from the sender actor to reconstruct the original task output
        generated by the sender actor.

        Args:
            dst_actor: The target actor to receive tensors
            task_args: List of arguments for the target actor task that may contain ObjectRefs.
        """
        from ray.experimental.channel import ChannelContext

        ctx = ChannelContext.get_current()

        actor_id_to_rank = {}
        for arg in task_args:
            # If an ObjectRef exists in `gpu_object_refs`, it means the ObjectRef
            # is in-actor tensors. Therefore, this function will trigger a tensor
            # communication operation between the sender and receiver actors.
            if not isinstance(arg, ObjectRef):
                continue

            if not self.is_gpu_object_ref(arg):
                continue
            tensor_meta = self.get_gpu_object_ref(arg)

            src_actor, tensor_meta = tensor_meta
            if not actor_id_to_rank:
                # TODO(kevin85421): Support multiple communicators.
                if len(ctx.communicators) != 1:
                    raise ValueError(
                        f"There are {len(ctx.communicators)} communicators in the current context. "
                        "Currently, GPU objects only support 1 communicator. Please make sure only "
                        "one communicator exists."
                    )
                actor_id_to_rank = {
                    a._ray_actor_id: i for i, a in enumerate(ctx.communicators[0])
                }
            if src_actor._ray_actor_id not in actor_id_to_rank:
                raise ValueError(
                    f"Sender actor {src_actor._ray_actor_id} not found in communicator. "
                    "Please make sure the sender and receiver are in the same communicator."
                )
            if dst_actor._ray_actor_id not in actor_id_to_rank:
                raise ValueError(
                    f"Receiver actor {dst_actor._ray_actor_id} not found in communicator. "
                    "Please make sure the sender and receiver are in the same communicator."
                )
            src_rank = actor_id_to_rank[src_actor._ray_actor_id]
            dst_rank = actor_id_to_rank[dst_actor._ray_actor_id]
            if src_rank == dst_rank:
                raise ValueError(
                    f"src_rank: {src_rank} and dst_rank: {dst_rank} are the same. This may cause deadlock for transports like NCCL."
                )
            src_actor.__ray_send__.remote(arg.hex(), dst_rank)
            dst_actor.__ray_recv__.remote(arg.hex(), src_rank, tensor_meta)
